{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a771bd25",
   "metadata": {},
   "source": [
    "# Sign Language Hand Detection and ROI Extraction\n",
    "\n",
    "This notebook implements hand detection and Region of Interest (ROI) extraction from sign language videos using MediaPipe and OpenCV.\n",
    "\n",
    "## Workflow Overview:\n",
    "1. **Load Data**: Load videos from the compressed videos dataset\n",
    "2. **Hand Detection**: Use MediaPipe to detect hands in each video frame\n",
    "3. **ROI Extraction**: Extract the region of interest (hands) from each frame\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8c6d5e",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "Make sure you have the following packages installed:\n",
    "- opencv-python\n",
    "- mediapipe\n",
    "- numpy\n",
    "- matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14b3fd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install opencv-python mediapipe numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdc8e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All libraries imported successfully!\n",
      "OpenCV version: 4.11.0\n",
      "MediaPipe version: 0.10.21\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "\n",
    "print(\"✓ All libraries imported successfully!\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"MediaPipe version: {mp.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b280e",
   "metadata": {},
   "source": [
    "## 2. Configuration and Path Setup\n",
    "\n",
    "Define paths to the dataset and configuration parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64841b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration complete!\n",
      "  - Videos directory: c:\\Users\\shifttech\\Desktop\\Univ_M\\AV\\Tp\\sign_Lang\\data\\compressed videos\n",
      "  - Output directory: c:\\Users\\shifttech\\Desktop\\Univ_M\\AV\\Tp\\sign_Lang\\output\n",
      "  - Max videos per word: 400\n",
      "  - Discovered 20 words: baby, eat, father, finish, good, happy, hear, house, important, love, mall, me, mosque, mother, normal, sad, stop, thanks, thinking, worry\n"
     ]
    }
   ],
   "source": [
    "# Path configurations\n",
    "BASE_DIR = Path(r'c:\\Users\\shifttech\\Desktop\\Univ_M\\AV\\Tp\\sign_Lang')\n",
    "DATA_DIR = BASE_DIR / 'data' / 'compressed videos'\n",
    "VIDEOS_DIR = DATA_DIR\n",
    "OUTPUT_DIR = BASE_DIR / 'output'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration parameters\n",
    "MIN_DETECTION_CONFIDENCE = 0.5  # Minimum confidence for hand detection\n",
    "MIN_TRACKING_CONFIDENCE = 0.5  # Minimum confidence for hand tracking\n",
    "MIN_HAND_PRESENCE_CONFIDENCE = 0.5  # Minimum confidence for hand presence\n",
    "MAX_VIDEOS_PER_WORD = 400  # Maximum number of videos to process per word\n",
    "\n",
    "# Automatically discover words from compressed videos directory\n",
    "TARGET_WORDS = []\n",
    "if VIDEOS_DIR.exists():\n",
    "    TARGET_WORDS = sorted([d.name for d in VIDEOS_DIR.iterdir() if d.is_dir()])\n",
    "\n",
    "print(f\"✓ Configuration complete!\")\n",
    "print(f\"  - Videos directory: {VIDEOS_DIR}\")\n",
    "\n",
    "print(f\"  - Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  - Max videos per word: {MAX_VIDEOS_PER_WORD}\")\n",
    "print(f\"  - Discovered {len(TARGET_WORDS)} words: {', '.join(TARGET_WORDS) if TARGET_WORDS else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3067f11",
   "metadata": {},
   "source": [
    "## 3. Load Dataset from Folder Structure\n",
    "\n",
    "Load videos from the compressed videos folder structure where each word has its own folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071f3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "LOADING AND SELECTING WORDS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "WORD SELECTION ANALYSIS\n",
      "============================================================\n",
      "Scanning directory: c:\\Users\\shifttech\\Desktop\\Univ_M\\AV\\Tp\\sign_Lang\\data\\compressed videos\n",
      "Words to process: 20\n",
      "Max videos per word: 400\n",
      "============================================================\n",
      "SELECTED WORDS (Video Files)\n",
      "============================================================\n",
      " 1. baby                 - ✓ 400 videos (limited from 430)\n",
      " 2. eat                  - ✓ 400 videos (limited from 440)\n",
      " 3. father               - ✓ 400 videos (limited from 452)\n",
      " 4. finish               - ✓ 400 videos (limited from 440)\n",
      " 5. good                 - ✓ 400 videos (limited from 436)\n",
      " 6. happy                - ✓ 400 videos (limited from 445)\n",
      " 7. hear                 - ✓ 400 videos (limited from 433)\n",
      " 8. house                - ✓ 400 videos (limited from 421)\n",
      " 9. important            - ✓ 400 videos (limited from 446)\n",
      "10. love                 - ✓ 400 videos (limited from 435)\n",
      "11. mall                 - ✓ 400 videos (limited from 414)\n",
      "12. me                   - ✓ 400 videos (limited from 430)\n",
      "13. mosque               - ✓ 400 videos (limited from 427)\n",
      "14. mother               - ✓ 400 videos (limited from 406)\n",
      "15. normal               - ✓ 400 videos (limited from 410)\n",
      "16. sad                  - ✓ 400 videos (limited from 420)\n",
      "17. stop                 - ✓ 400 videos (limited from 426)\n",
      "18. thanks               - ✓ 400 videos (limited from 412)\n",
      "19. thinking             - ✓ 366 videos\n",
      "20. worry                - ✓ 349 videos\n",
      "\n",
      "============================================================\n",
      "Total: 20 words, 7915 videos\n",
      "============================================================\n",
      "\n",
      "✓ Dataset loaded and words selected successfully!\n"
     ]
    }
   ],
   "source": [
    "def select_words_from_folders(videos_dir: Path, \n",
    "                              target_glosses: List[str] = None,\n",
    "                              max_videos_per_word: int = 200) -> Dict:\n",
    "    \"\"\"\n",
    "    Select words from folder-based dataset structure.\n",
    "    Each word has its own folder containing video files directly.\n",
    "    \n",
    "    Args:\n",
    "        videos_dir: Directory containing word folders\n",
    "        target_glosses: List of specific words to select (if None, selects all)\n",
    "        max_videos_per_word: Maximum number of videos to select per word\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with word information and video file paths\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'selected_words': [],\n",
    "        'word_to_videos': {},\n",
    "        'total_videos': 0\n",
    "    }\n",
    "    \n",
    "    # If no target glosses specified, discover all word folders\n",
    "    if target_glosses is None or len(target_glosses) == 0:\n",
    "        if not videos_dir.exists():\n",
    "            print(f\"✗ Videos directory does not exist: {videos_dir}\")\n",
    "            return result\n",
    "        target_glosses = sorted([d.name for d in videos_dir.iterdir() if d.is_dir()])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"WORD SELECTION ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Scanning directory: {videos_dir}\")\n",
    "    print(f\"Words to process: {len(target_glosses)}\")\n",
    "    print(f\"Max videos per word: {max_videos_per_word}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"SELECTED WORDS (Video Files)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    words_not_found = []\n",
    "    words_no_videos = []\n",
    "    \n",
    "    for idx, gloss in enumerate(target_glosses, 1):\n",
    "        # Check if word folder exists\n",
    "        word_folder = videos_dir / gloss\n",
    "        if not word_folder.exists():\n",
    "            words_not_found.append(gloss)\n",
    "            print(f\"{idx:2d}. {gloss:20s} - ✗ NOT FOUND (folder doesn't exist)\")\n",
    "            continue\n",
    "        \n",
    "        # Get all video files in the word folder\n",
    "        all_video_files = list(word_folder.glob('*.mp4')) + list(word_folder.glob('*.avi')) + list(word_folder.glob('*.mov'))\n",
    "        \n",
    "        if len(all_video_files) == 0:\n",
    "            words_no_videos.append(gloss)\n",
    "            print(f\"{idx:2d}. {gloss:20s} - ✗ No video files found in folder\")\n",
    "            continue\n",
    "        \n",
    "        # Limit to max_videos_per_word\n",
    "        video_files = all_video_files[:max_videos_per_word]\n",
    "        \n",
    "        # Store video file paths\n",
    "        result['selected_words'].append(gloss)\n",
    "        result['word_to_videos'][gloss] = {\n",
    "            'video_paths': video_files,\n",
    "            'video_count': len(video_files)\n",
    "        }\n",
    "        result['total_videos'] += len(video_files)\n",
    "        \n",
    "        if len(all_video_files) > max_videos_per_word:\n",
    "            print(f\"{idx:2d}. {gloss:20s} - ✓ {len(video_files)} videos (limited from {len(all_video_files)})\")\n",
    "        else:\n",
    "            print(f\"{idx:2d}. {gloss:20s} - ✓ {len(video_files)} videos\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total: {len(result['selected_words'])} words, {result['total_videos']} videos\")\n",
    "    \n",
    "    if words_not_found:\n",
    "        print(f\"\\n⚠ Words not found: {', '.join(words_not_found)}\")\n",
    "    if words_no_videos:\n",
    "        print(f\"⚠ Words with no video files: {', '.join(words_no_videos)}\")\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LOADING AND SELECTING WORDS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Select words from folder structure\n",
    "selection_result = select_words_from_folders(\n",
    "    VIDEOS_DIR,\n",
    "    target_glosses=TARGET_WORDS,\n",
    "    max_videos_per_word=MAX_VIDEOS_PER_WORD\n",
    ")\n",
    "\n",
    "# Extract data for processing\n",
    "target_words = selection_result['selected_words']\n",
    "word_to_videos = selection_result['word_to_videos']\n",
    "\n",
    "print(f\"\\n✓ Dataset loaded and words selected successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89651c3",
   "metadata": {},
   "source": [
    "## 4. Step 2: Hand Detection with MediaPipe\n",
    "\n",
    "Initialize MediaPipe Hands solution and create a function to detect hands in video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "981ca23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ MediaPipe Hands initialized successfully!\n",
      "  - Max hands: 2\n",
      "  - Detection confidence: 0.5\n",
      "  - Tracking confidence: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Create Hands detector\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=MIN_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MIN_TRACKING_CONFIDENCE\n",
    ")\n",
    "\n",
    "print(\"✓ MediaPipe Hands initialized successfully!\")\n",
    "print(f\"  - Max hands: 2\")\n",
    "print(f\"  - Detection confidence: {MIN_DETECTION_CONFIDENCE}\")\n",
    "print(f\"  - Tracking confidence: {MIN_TRACKING_CONFIDENCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddf2eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Hand detection functions defined!\n"
     ]
    }
   ],
   "source": [
    "def detect_hands_in_frame(frame: np.ndarray, hands_detector) -> Tuple[np.ndarray, any]:\n",
    "    \"\"\"\n",
    "    Detect hands in a single frame using MediaPipe.\n",
    "    \n",
    "    Args:\n",
    "        frame: Input frame (BGR format)\n",
    "        hands_detector: MediaPipe Hands detector\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (processed frame in RGB, detection results)\n",
    "    \"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process the frame\n",
    "    results = hands_detector.process(frame_rgb)\n",
    "    \n",
    "    return frame_rgb, results\n",
    "\n",
    "\n",
    "def draw_hand_landmarks(frame: np.ndarray, results) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw hand landmarks on the frame.\n",
    "    \n",
    "    Args:\n",
    "        frame: Input frame (RGB format)\n",
    "        results: MediaPipe detection results\n",
    "    \n",
    "    Returns:\n",
    "        Frame with drawn landmarks\n",
    "    \"\"\"\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                annotated_frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "    \n",
    "    return annotated_frame\n",
    "\n",
    "\n",
    "print(\"✓ Hand detection functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375ca1f",
   "metadata": {},
   "source": [
    "## 5. Step 3: ROI Extraction\n",
    "\n",
    "Extract the Region of Interest (ROI) containing the detected hands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5434e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ROI extraction functions defined!\n"
     ]
    }
   ],
   "source": [
    "def get_hand_bounding_box(hand_landmarks, frame_width: int, frame_height: int, \n",
    "                          padding: float = 0.1) -> Tuple[int, int, int, int]:\n",
    "    \"\"\"\n",
    "    Calculate bounding box for a hand with padding.\n",
    "    \n",
    "    Args:\n",
    "        hand_landmarks: MediaPipe hand landmarks\n",
    "        frame_width: Width of the frame\n",
    "        frame_height: Height of the frame\n",
    "        padding: Padding ratio around the hand (0.1 = 10%)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (x_min, y_min, x_max, y_max)\n",
    "    \"\"\"\n",
    "    # Get all landmark coordinates\n",
    "    x_coords = [lm.x for lm in hand_landmarks.landmark]\n",
    "    y_coords = [lm.y for lm in hand_landmarks.landmark]\n",
    "    \n",
    "    # Calculate bounding box\n",
    "    x_min = min(x_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    y_max = max(y_coords)\n",
    "    \n",
    "    # Add padding\n",
    "    width = x_max - x_min\n",
    "    height = y_max - y_min\n",
    "    x_min = max(0, x_min - width * padding)\n",
    "    x_max = min(1, x_max + width * padding)\n",
    "    y_min = max(0, y_min - height * padding)\n",
    "    y_max = min(1, y_max + height * padding)\n",
    "    \n",
    "    # Convert to pixel coordinates\n",
    "    x_min_px = int(x_min * frame_width)\n",
    "    x_max_px = int(x_max * frame_width)\n",
    "    y_min_px = int(y_min * frame_height)\n",
    "    y_max_px = int(y_max * frame_height)\n",
    "    \n",
    "    return x_min_px, y_min_px, x_max_px, y_max_px\n",
    "\n",
    "\n",
    "def extract_roi(frame: np.ndarray, results, padding: float = 0.1) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extract ROI (Region of Interest) for each detected hand.\n",
    "    \n",
    "    Args:\n",
    "        frame: Input frame (RGB format)\n",
    "        results: MediaPipe detection results\n",
    "        padding: Padding ratio around the hand\n",
    "    \n",
    "    Returns:\n",
    "        List of cropped hand images\n",
    "    \"\"\"\n",
    "    rois = []\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get bounding box\n",
    "            x_min, y_min, x_max, y_max = get_hand_bounding_box(\n",
    "                hand_landmarks, frame_width, frame_height, padding\n",
    "            )\n",
    "            \n",
    "            # Extract ROI\n",
    "            roi = frame[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "            if roi.size > 0:  # Check if ROI is valid\n",
    "                rois.append(roi)\n",
    "    \n",
    "    return rois\n",
    "\n",
    "\n",
    "def draw_bounding_boxes(frame: np.ndarray, results, padding: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes around detected hands.\n",
    "    \n",
    "    Args:\n",
    "        frame: Input frame (RGB format)\n",
    "        results: MediaPipe detection results\n",
    "        padding: Padding ratio around the hand\n",
    "    \n",
    "    Returns:\n",
    "        Frame with bounding boxes drawn\n",
    "    \"\"\"\n",
    "    annotated_frame = frame.copy()\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        frame_height, frame_width = frame.shape[:2]\n",
    "        \n",
    "        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):\n",
    "            # Get bounding box\n",
    "            x_min, y_min, x_max, y_max = get_hand_bounding_box(\n",
    "                hand_landmarks, frame_width, frame_height, padding\n",
    "            )\n",
    "            \n",
    "            # Draw rectangle\n",
    "            color = (0, 255, 0) if idx == 0 else (255, 0, 0)  # Green for first hand, red for second\n",
    "            cv2.rectangle(annotated_frame, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "            \n",
    "            # Add label\n",
    "            label = f\"Hand {idx + 1}\"\n",
    "            cv2.putText(annotated_frame, label, (x_min, y_min - 10),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    return annotated_frame\n",
    "\n",
    "\n",
    "print(\"✓ ROI extraction functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa21ecd",
   "metadata": {},
   "source": [
    "## 6. Process Videos\n",
    "\n",
    "Process each video to detect hands and extract ROIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28c420ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Video processing function defined!\n"
     ]
    }
   ],
   "source": [
    "def process_video(video_path: Path, hands_detector, output_dir: Path = None, \n",
    "                 max_frames: int = None, visualize: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Process a single video for hand detection and ROI extraction.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "        hands_detector: MediaPipe Hands detector\n",
    "        output_dir: Directory to save output (optional)\n",
    "        max_frames: Maximum number of frames to process (optional)\n",
    "        visualize: Whether to save visualization frames\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing processing results\n",
    "    \"\"\"\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(f\"  ✗ Error: Could not open video {video_path.name}\")\n",
    "        return None\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Limit frames if specified\n",
    "    frames_to_process = min(total_frames, max_frames) if max_frames else total_frames\n",
    "    \n",
    "    # Results storage\n",
    "    results_data = {\n",
    "        'video_name': video_path.name,\n",
    "        'fps': fps,\n",
    "        'total_frames': total_frames,\n",
    "        'processed_frames': 0,\n",
    "        'frames_with_hands': 0,\n",
    "        'rois_extracted': 0\n",
    "    }\n",
    "    \n",
    "    # Create output directory for this video if needed\n",
    "    if output_dir and visualize:\n",
    "        video_output_dir = output_dir / video_path.stem\n",
    "        video_output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    frame_idx = 0\n",
    "    \n",
    "    while cap.isOpened() and frame_idx < frames_to_process:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detect hands\n",
    "        frame_rgb, detection_results = detect_hands_in_frame(frame, hands_detector)\n",
    "        \n",
    "        # Extract ROIs\n",
    "        rois = extract_roi(frame_rgb, detection_results)\n",
    "        \n",
    "        # Update statistics\n",
    "        results_data['processed_frames'] += 1\n",
    "        if detection_results.multi_hand_landmarks:\n",
    "            results_data['frames_with_hands'] += 1\n",
    "            results_data['rois_extracted'] += len(rois)\n",
    "        \n",
    "        # Save hand ROIs for every frame (needed for training)\n",
    "        if visualize and output_dir and len(rois) > 0:\n",
    "            # Save individual hand ROIs for each frame\n",
    "            for roi_idx, roi in enumerate(rois):\n",
    "                roi_path = video_output_dir / f\"frame_{frame_idx:04d}_hand_{roi_idx}.jpg\"\n",
    "                cv2.imwrite(str(roi_path), cv2.cvtColor(roi, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # Save annotated visualization frames (every 10th frame to save space)\n",
    "        if visualize and output_dir and frame_idx % 10 == 0:\n",
    "            # Draw landmarks and bounding boxes\n",
    "            annotated_frame = draw_hand_landmarks(frame_rgb, detection_results)\n",
    "            annotated_frame = draw_bounding_boxes(annotated_frame, detection_results)\n",
    "            \n",
    "            # Save annotated frame\n",
    "            output_path = video_output_dir / f\"frame_{frame_idx:04d}.jpg\"\n",
    "            cv2.imwrite(str(output_path), cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    return results_data\n",
    "\n",
    "\n",
    "print(\"✓ Video processing function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b33fbf",
   "metadata": {},
   "source": [
    "## 7. Process All Selected Videos\n",
    "\n",
    "Process all selected videos and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b363fe26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING VIDEOS\n",
      "============================================================\n",
      "\n",
      "Processing class: baby (400 videos)\n",
      "[░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░] 0.1% (10/7915)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbar\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprogress\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_videos\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Process video (all frames, save output for all videos)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m results = \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Process all frames\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save hand ROIs for all videos\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m results:\n\u001b[32m     39\u001b[39m     all_results.append(results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mprocess_video\u001b[39m\u001b[34m(video_path, hands_detector, output_dir, max_frames, visualize)\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Detect hands\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m frame_rgb, detection_results = \u001b[43mdetect_hands_in_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhands_detector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# Extract ROIs\u001b[39;00m\n\u001b[32m     59\u001b[39m rois = extract_roi(frame_rgb, detection_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mdetect_hands_in_frame\u001b[39m\u001b[34m(frame, hands_detector)\u001b[39m\n\u001b[32m     13\u001b[39m frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Process the frame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m results = \u001b[43mhands_detector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_rgb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frame_rgb, results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shifttech\\Desktop\\Univ_M\\AV\\Tp\\sign_Lang\\.venv\\Lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[39m, in \u001b[36mHands.process\u001b[39m\u001b[34m(self, image)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np.ndarray) -> NamedTuple:\n\u001b[32m    133\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[32m    134\u001b[39m \n\u001b[32m    135\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \u001b[33;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[32m    151\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shifttech\\Desktop\\Univ_M\\AV\\Tp\\sign_Lang\\.venv\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001b[39m, in \u001b[36mSolutionBase.process\u001b[39m\u001b[34m(self, input_data)\u001b[39m\n\u001b[32m    334\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m._graph.add_packet_to_input_stream(\n\u001b[32m    336\u001b[39m         stream=stream_name,\n\u001b[32m    337\u001b[39m         packet=\u001b[38;5;28mself\u001b[39m._make_packet(input_stream_type,\n\u001b[32m    338\u001b[39m                                  data).at(\u001b[38;5;28mself\u001b[39m._simulated_timestamp))\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._output_stream_type_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Process all selected videos\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING VIDEOS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_results = []\n",
    "video_count = 0\n",
    "total_videos = sum(len(word_to_videos[word]['video_paths']) for word in target_words)\n",
    "\n",
    "# Process videos from folder structure\n",
    "for word in target_words:\n",
    "    word_data = word_to_videos.get(word)\n",
    "    if not word_data:\n",
    "        continue\n",
    "    \n",
    "    video_paths = word_data['video_paths']\n",
    "    print(f\"\\nProcessing class: {word} ({len(video_paths)} videos)\")\n",
    "    \n",
    "    for video_path in video_paths:\n",
    "        video_count += 1\n",
    "        \n",
    "        # Show progress bar\n",
    "        progress = video_count / total_videos * 100\n",
    "        bar_length = 50\n",
    "        filled = int(bar_length * video_count / total_videos)\n",
    "        bar = '█' * filled + '░' * (bar_length - filled)\n",
    "        print(f\"\\r[{bar}] {progress:.1f}% ({video_count}/{total_videos})\", end='', flush=True)\n",
    "        \n",
    "        # Process video (all frames, save output for all videos)\n",
    "        results = process_video(\n",
    "            video_path, \n",
    "            hands, \n",
    "            output_dir=OUTPUT_DIR,\n",
    "            max_frames=None,  # Process all frames\n",
    "            visualize=True  # Save hand ROIs for all videos\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            all_results.append(results)\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29888fd",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics and Visualization\n",
    "\n",
    "Display overall statistics and sample visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics\n",
    "if all_results:\n",
    "    total_frames = sum(r['processed_frames'] for r in all_results)\n",
    "    total_frames_with_hands = sum(r['frames_with_hands'] for r in all_results)\n",
    "    total_rois = sum(r['rois_extracted'] for r in all_results)\n",
    "    avg_detection_rate = (total_frames_with_hands / total_frames * 100) if total_frames > 0 else 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total videos processed: {len(all_results)}\")\n",
    "    print(f\"Total frames processed: {total_frames}\")\n",
    "    print(f\"Frames with hands detected: {total_frames_with_hands} ({avg_detection_rate:.1f}%)\")\n",
    "    print(f\"Total ROIs extracted: {total_rois}\")\n",
    "    print(f\"Average ROIs per frame with hands: {total_rois / total_frames_with_hands:.2f}\" \n",
    "          if total_frames_with_hands > 0 else \"N/A\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bf5632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample results\n",
    "print(\"\\nVisualizing sample results...\")\n",
    "\n",
    "# Find the first video output directory with saved frames\n",
    "sample_dirs = [d for d in OUTPUT_DIR.iterdir() if d.is_dir()]\n",
    "\n",
    "if sample_dirs:\n",
    "    sample_dir = sample_dirs[0]\n",
    "    frame_files = sorted([f for f in sample_dir.glob('frame_*.jpg') if '_hand_' not in f.name])[:4]\n",
    "    \n",
    "    if frame_files:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, frame_file in enumerate(frame_files):\n",
    "            img = cv2.imread(str(frame_file))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[idx].imshow(img_rgb)\n",
    "            axes[idx].set_title(f'Frame {frame_file.stem}', fontsize=12)\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Sample Hand Detection Results - {sample_dir.name}', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"✓ Visualization complete! Check the output directory: {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        print(\"  No sample frames found for visualization.\")\n",
    "else:\n",
    "    print(\"  No output directories found. Set visualize=True when processing videos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e08c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the image.png\n",
    "image_path = \"./image.png\"\n",
    "img = cv2.imread(image_path)\n",
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(img_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7b738a",
   "metadata": {},
   "source": [
    "## 9. Display Sample ROIs\n",
    "\n",
    "Show extracted hand ROIs from the first video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565db003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample ROIs\n",
    "if sample_dirs:\n",
    "    sample_dir = sample_dirs[0]\n",
    "    roi_files = sorted(sample_dir.glob('*_hand_*.jpg'))[:6]\n",
    "    \n",
    "    if roi_files:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, roi_file in enumerate(roi_files):\n",
    "            img = cv2.imread(str(roi_file))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            axes[idx].imshow(img_rgb)\n",
    "            axes[idx].set_title(f'{roi_file.stem}', fontsize=10)\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for idx in range(len(roi_files), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle('Sample Extracted Hand ROIs', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✓ ROI visualization complete!\")\n",
    "    else:\n",
    "        print(\"  No ROI files found for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8393d83",
   "metadata": {},
   "source": [
    "## 10. Cleanup\n",
    "\n",
    "Release resources and clean up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4468d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close MediaPipe Hands\n",
    "hands.close()\n",
    "\n",
    "print(\"\\n✓ All resources released successfully!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK EXECUTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nOutput saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the extracted ROIs in the output directory\")\n",
    "print(\"2. Use these ROIs for further processing (e.g., feature extraction, training)\")\n",
    "print(\"3. Adjust parameters (NUM_WORDS, MIN_VIDEOS_PER_WORD, etc.) as needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "working venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
